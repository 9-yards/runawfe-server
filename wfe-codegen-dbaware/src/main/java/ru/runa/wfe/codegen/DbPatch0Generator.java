package ru.runa.wfe.codegen;

import java.io.File;
import java.io.FileWriter;
import lombok.val;
import lombok.var;
import ru.runa.wfe.codegen.DbStructureAnalyzer.Structure;

class DbPatch0Generator {

    static void generate(Structure st, File f) throws Exception {
        val w = new FileWriter(f);
        w.write("//\n" +
                "// ATTENTION!!! This file is auto-generated by wfe-codegen-dbaware subproject. See README.txt there.\n" +
                "//\n" +
                "package ru.runa.wfe.commons.dbpatch;\n" +
                "\n" +
                "import com.google.common.collect.ImmutableList;\n" +
                "import java.sql.Connection;\n");
        if (!st.migrations.isEmpty()) {
            w.write("import java.sql.PreparedStatement;\n");
        }
        w.write("import java.sql.Timestamp;\n" +
                "import java.util.List;\n" +
                "\n" +
                "public class DbPatch0 extends DbPatch {\n" +
                "\n" +
                "    @Override\n" +
                "    protected List<String> getDDLQueriesBefore() {\n" +
                "        return ImmutableList.of(");


        var firstTable = true;
        for (val t : st.tables) {
            if (firstTable) {
                firstTable = false;
                w.write("\n");
            } else {
                w.write(",\n");
            }
            w.write("                getDDLCreateTable(\"" + t.name + "\", ImmutableList.of(");
            var firstColumn = true;
            for (val c : t.columns) {
                if (firstColumn) {
                    firstColumn = false;
                    w.write("\n");
                } else {
                    w.write(",\n");
                }
                w.write("                        new ");

                val quotedName = "\"" + c.name + "\"";
                val allowNulls = c.isNotNull ? "false" : "true";
                switch (c.type) {
                    case BIGINT:
                        w.write("BigintColumnDef(" + quotedName + ", " + allowNulls + ")");
                        break;
                    case BLOB:
                        w.write("BlobColumnDef(" + quotedName + ", " + allowNulls + ")");
                        break;
                    case BOOLEAN:
                        w.write("BooleanColumnDef(" + quotedName + ", " + allowNulls + ")");
                        break;
                    case CHAR:
                        w.write("CharColumnDef(" + quotedName + ", " + c.typeLength + ", " + allowNulls + ")");
                        break;
                    case DOUBLE:
                        w.write("DoubleColumnDef(" + quotedName + ", " + allowNulls + ")");
                        break;
                    case INT:
                        w.write("IntColumnDef(" + quotedName + ", " + allowNulls + ")");
                        break;
                    case TIMESTAMP:
                        w.write("TimestampColumnDef(" + quotedName + ", " + allowNulls + ")");
                        break;
                    case VARCHAR:
                        w.write("VarcharColumnDef(" + quotedName + ", " + c.typeLength + ", " + allowNulls + ")");
                        break;
                    default:
                        throw new Exception("Internal error: unhandled column type " + c.type);
                }
                if (c.isPrimaryKey) {
                    w.write(".setPrimaryKey()");
                }
            }
            w.write("\n" +
                    "                ))");
        }


        // Creating indexes before UK and FK, to avoid automatic index creation by SQL server.
        for (val i : st.indexes) {
            w.write(",\n" +
                    "                getDDLCreateIndex(\"" + i.table.name + "\", \"" + i.name + "\"");
            for (val c : i.columns) {
                w.write(", \"" + c.name + "\"");
            }
            w.write(")");
        }


        for (val uk : st.uniqueKeys) {
            w.write(",\n" +
                    "                getDDLCreateUniqueKey(\"" + uk.table.name + "\", \"" + uk.constraintName + "\"");
            for (val c : uk.columns) {
                w.write(", \"" + c.name + "\"");
            }
            w.write(")");
        }


        for (val fk : st.foreignKeys) {
            w.write(",\n" +
                    "                getDDLCreateForeignKey(\"" + fk.table.name + "\", \"" + fk.constraintName + "\", \"" + fk.column.name +
                    "\", \"" + fk.refTable.name + "\", \"" + fk.refColumn.name + "\")");
        }


        w.write("\n        );\n" +
                "    }\n");
        if (!st.migrations.isEmpty()) {
            w.write("\n" +
                    "    @Override\n" +
                    "    public void executeDML(Connection conn) throws Exception {\n" +
                    "        try (PreparedStatement stmt = conn.prepareStatement(\"insert into db_migration(name, when_started_when_finished) values(?, ?, ?)\")) {\n");
            for (val m : st.migrations) {
                // TODO I wonder if timezone will interfere so each run we'll get more and more shifted time.
                w.write("            insertMigration(stmt, \"" + m.name + "\", " + m.whenStarted.getTime() + "L, " +
                        (m.whenFinished == null ? "null" : m.whenFinished.getTime() + "L") + ");\n");
            }
            w.write("        }\n" +
                    "    }\n" +
                    "\n" +
                    "    private void insertMigration(PreparedStatement stmt, String name, long whenStarted, Long whenFinished) throws Exception {\n" +
                    "        stmt.setString(1, name);\n" +
                    "        stmt.setTimestamp(2, new Timestamp(whenStarted));\n" +
                    "        stmt.setTimestamp(3, whenFinished == null ? null : new Timestamp(whenFinished));\n" +
                    "        stmt.executeUpdate();\n" +
                    "    }\n");
        }
        w.write("}\n");
        w.close();
    }
}
